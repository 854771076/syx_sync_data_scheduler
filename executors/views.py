from django.shortcuts import render
from .models import Log
from django.db.models import Count
from rest_framework.views import APIView
from django.db.models import ExpressionWrapper, DurationField
from rest_framework.response import Response
from rest_framework import status
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from django_apscheduler.jobstores import DjangoJobStore
from django.db import models
from django.shortcuts import redirect
# from django.utils import timezone
from django.db.models import Q, Avg, F
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
import psutil
from datetime import datetime
# from django.utils import timezone
from executors.alerts import AlertFactory
from loguru import logger
from executors.extensions import ManagerFactory
from executors.models import *
# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

scheduler = BackgroundScheduler({
    'apscheduler.executors.default': {
        'class': 'apscheduler.executors.pool:ThreadPoolExecutor',
        'max_workers': int(os.environ.get('SCHEDULER_MAX_WORKERS', 1000))
    }
})

scheduler.add_jobstore(DjangoJobStore(), "default")
scheduler.start()


def execute_datax_tasks(project,settings):
    try:
        alert=AlertFactory(project.notification)
        
        tasks = Task.objects.filter(is_active=True, project=project)
        manager = ManagerFactory(project.engine)
        manager = manager(tasks, settings=settings)
        start_time=datetime.now()
        alert.send_message(**{
            "name":project.name,
            "partition_date":manager.settings.get('partition_date'),
            'start_time':start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'end_time':None,
            "content":f"åŒæ­¥å¼€å§‹"
        })
        
        manager.execute_tasks()
        end_time=datetime.now()
        alert.send_message(**{
            "name":project.name,
            "partition_date":manager.settings.get('partition_date'),
            'start_time':start_time.strftime('%Y-%m-%d %H:%M:%S'),
            'end_time':end_time.strftime('%Y-%m-%d %H:%M:%S'),
            "content":f"åŒæ­¥å®Œæˆ"
        })

    except Exception as e:
        logger.exception(e)
# æ—¥å¿—æ£€æŸ¥,å¹¶é‡è¯•
def check_logs_and_retry():
    from executors.extensions.datax.datax_plugin import DataXPluginManager
    try:
        # æŸ¥è¯¢å‡ºå½“å¤©å¤±è´¥æ—¥å¿—
        today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        logs = Log.objects.filter(complit_state=0, created_at__gte=today_start)
        if not logs:
            logger.debug("No failed logs found today.")
            return
        logger.warning(f"Found {logs.count()} failed logs today.")
        DataXPluginManager.execute_retry(logs)
    except Exception as e:
        logger.exception(e)
# æ—¥å¿—æ£€æŸ¥
def check_logs():
    try:
        today_start = datetime.now().date()
        projects = Project.objects.filter(is_active=True)
        msg = f"""### æ¯æ—¥é¡¹ç›®æ‰§è¡Œæƒ…å†µç»Ÿè®¡
#### æ—¥æœŸï¼š{today_start}
âœ… : æˆåŠŸ âŒ : å¤±è´¥ â³ : æ‰§è¡Œä¸­  ğŸ’¾ : å¤‡ä»½ â¸ï¸ : åœæ­¢

"""
        
        for project in projects:
            today_logs = Log.objects.filter(
                task__project=project,
                created_at__date=today_start,
            )

            total_tasks = Task.objects.filter(project=project,is_active=True).count()
            # ç»Ÿè®¡å„çŠ¶æ€ä»»åŠ¡æ•°é‡
            status_stats = {
                'success': today_logs.filter(complit_state=1).count(),
                'fail': today_logs.filter(complit_state=0).count(),
                'process': today_logs.filter(complit_state=2).count(),
                'bak': today_logs.filter(complit_state=3).count(),
                'stopped': today_logs.filter(complit_state=4).count(),
                'total': total_tasks,
            }
            
            # è®¡ç®—è€—æ—¶
            duration = ""
            if today_logs.exists():
                start_time = today_logs.earliest('created_at').created_at
                end_time = today_logs.latest('updated_at').updated_at
                duration = str(end_time - start_time).split('.')[0]  # å»é™¤æ¯«ç§’éƒ¨åˆ†
            
            # åˆ¤æ–­é¡¹ç›®æ˜¯å¦å®Œæˆ
            if status_stats['success'] >= status_stats['total'] :
                status='å·²å®Œæˆ'
            elif status_stats['process'] > 0 or status_stats['fail'] > 0:
                status='è¿›è¡Œä¸­/æœ‰å¤±è´¥'
            elif status_stats['stopped'] > 0:
                status='æœ‰åœæ­¢'
            elif status_stats['total'] > 0 and today_logs.count()==0:
                status='æœªæ‰§è¡Œ'
            msg += f"""**{project.name}**

- âœ… : {status_stats['success']} âŒ : {status_stats['fail']} â³ : {status_stats['process']}  ğŸ’¾ : {status_stats['bak']} â¸ï¸ : {status_stats['stopped']}
- ğŸ”¢ **æ€»ä»»åŠ¡æ•°**: **{status_stats['total']}**
- â±ï¸ **æ€»è€—æ—¶**: **{duration if duration else "æ— æ•°æ®"}**
- ğŸ **çŠ¶æ€**: **{status}**
\n"""
        # æŸ¥è¯¢å‡ºå½“å¤©å¤±è´¥æ—¥å¿—
        # today_start = datetime.now().date()
        # # æ ¹æ®é¡¹ç›®æ¯æ—¥ç»Ÿè®¡æƒ…å†µï¼Œå‘é€é’‰é’‰æ¶ˆæ¯
        # projects = Project.objects.filter(is_active=True)
        # msg=f"### æ¯æ—¥é¡¹ç›®æ‰§è¡Œæƒ…å†µç»Ÿè®¡ \n #### æ—¥æœŸï¼š{today_start} \n | é¡¹ç›®åç§° | æ‰§è¡Œæ¬¡æ•° | å¤±è´¥æ¬¡æ•° | æˆåŠŸç‡ | å¼€å§‹æ—¶é—´ | ç»“æŸæ—¶é—´ |\n | --- | --- | --- | --- | --- | --- |\n"
        
        # for project in projects:
        #     # è·å–ä»Šå¤©çš„æ—¥å¿—
        #     today_logs = Log.objects.filter(
        #         task__project=project,
        #         created_at__date=today_start,
        #     )
        #     # ç»Ÿè®¡æ‰§è¡Œæ¬¡æ•°å’Œå¤±è´¥æ¬¡æ•°
        #     total_executions = today_logs.count()
        #     # æ‰§è¡Œä¸­
        #     failed_executions = today_logs.filter(complit_state=0).count()
        #     start_time = today_logs.earliest('created_at').created_at if today_logs.exists() else ''
        #     end_time = today_logs.latest('created_at').created_at if today_logs.exists() else ''

        #     # è®¡ç®—æˆåŠŸç‡
        #     success_rate = (total_executions - failed_executions) / total_executions * 100 if total_executions > 0 else 0
        #     # æ„å»ºæ¶ˆæ¯
        #     msg += f"| {project.name} | {total_executions} | {failed_executions} | {success_rate:.2f}% | {start_time} | {end_time} |\n"
        # é»˜è®¤é€šçŸ¥
        notification=Notification.objects.get(name='é»˜è®¤')
        AlertFactory(notification,'æ—¥å¸¸ä»»åŠ¡æ‰§è¡Œæƒ…å†µç»Ÿè®¡').send_custom_message(msg)
        
    except Exception as e:
        logger.exception(e)

# å¤‡ä»½é™¤æ—¥å¿—å¤–çš„æ‰€æœ‰è¡¨æ•°æ®ä¸ºsqlæ–‡ä»¶
def backup_data():
    from django.apps import apps 
    import shutil
    try:
        # è·å–æ‰€æœ‰è¡¨å
        tables = apps.get_models()
        # ä»Šå¤©æ—¥æœŸ
        today = datetime.now().date()
        # æ„å»ºå¤‡ä»½æ–‡ä»¶è·¯å¾„
        backup_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'backups', today.strftime('%Y-%m-%d'))
        os.makedirs(backup_dir, exist_ok=True)
        config=dict(ConfigItem.objects.all().values_list("key", "value"))
        # djangoé¡¹ç›®ç»å¯¹è·¯å¾„
        project_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        backup_file = os.path.join(backup_dir, f'executors.json')
        # æ‰§è¡Œå¤‡ä»½å‘½ä»¤
        python_path=config.get('PYTHON_BIN_PATH')
        os.system(f'{python_path} {project_path}/manage.py dumpdata --exclude executors.log --exclude executors.metadatatable > {backup_file}')
        logger.info(f'Backup of executors completed.')
        # åˆ é™¤ä¸ƒå¤©å‰å¤‡ä»½ç›®å½•
        days_ago = datetime.now() - timedelta(days=30)
        for dir in os.listdir(os.path.dirname(backup_dir)):
            dir_path = os.path.join(os.path.dirname(backup_dir), dir)
            if dir < days_ago.strftime('%Y-%m-%d') and os.path.isdir(dir_path):
                shutil.rmtree(dir_path)  # ä½¿ç”¨shutil.rmtreeåˆ é™¤éç©ºç›®å½•
                logger.info(f'Deleted old backup directory: {dir_path}')
    except Exception as e:
        logger.exception(e)

#åˆ é™¤30å¤©å‰æ‰€æœ‰æˆåŠŸçš„æ—¥å¿—
def delete_old_logs():
    import shutil
    try:
        # è®¡ç®—30å¤©å‰çš„æ—¥æœŸ
        days_ago = datetime.now() - timedelta(days=30)
        # åˆ é™¤30å¤©å‰çš„æˆåŠŸæ—¥å¿—
        Log.objects.filter(complit_state=1, created_at__lt=days_ago).delete()
        # åˆ é™¤30å¤©å‰çš„æœ¬åœ°æ—¥å¿—ç›®å½•
        log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'static', 'logs')
        for dir in os.listdir(log_dir):
            dir_path = os.path.join(log_dir, dir)
            if dir < days_ago.strftime('%Y%m%d') and os.path.isdir(dir_path):
                shutil.rmtree(dir_path)  # ä½¿ç”¨shutil.rmtreeåˆ é™¤éç©ºç›®å½•
                logger.info(f'Deleted old backup directory: {dir_path}')
        logger.info('Old logs deleted successfully.')
    except Exception as e:
        logger.exception(e)


def init_scheduler_task():
    project=Project.objects.filter(is_active=True)
    for p in project:
        settings={
            "execute_way": "update",
            **p.config
        }
        if not p.config.get('cron'):
            logger.warning(f"Project {p.name} has no cron expression. Skipping.")
            continue
        trigger = CronTrigger.from_crontab(p.config['cron'])
        scheduler.add_job(
            execute_datax_tasks,
            args=[p, settings],
            trigger=trigger,
            id=f"datax_task_{p.name}",
            replace_existing=True,
            max_instances=10,
            misfire_grace_time=60*60*24
        )
        logger.info(f"DataX scheduler started successfully for project {p.name}")

    # æ£€æŸ¥æ—¥å¿—
    scheduler.add_job(
        check_logs,
        trigger=CronTrigger.from_crontab('00 9,12,23 * * *'),
        id='æ£€æŸ¥æ—¥å¿—', 
        replace_existing=True,
        max_instances=10,
            misfire_grace_time=60*60*24
    )
    scheduler.add_job(
        update_all_tasks_metadata,
        # æ¯å¤©æ‰§è¡Œä¸€æ¬¡
        trigger=CronTrigger.from_crontab('0 21 * * *'),
        id='æ›´æ–°å…ƒæ•°æ®', 
        replace_existing=True,
        max_instances=10,
        misfire_grace_time=60*60*24
    )
    # scheduler.add_job(
    #     delete_old_logs,
    #     # æ¯å¤©æ‰§è¡Œä¸€æ¬¡
    #     trigger=CronTrigger.from_crontab('0 22 * * *'),
    #     id='åˆ é™¤30å¤©å‰æˆåŠŸçš„æ—¥å¿—', 
    #     replace_existing=True,
    #     max_instances=10,
    #     misfire_grace_time=60*60*24
    # )
    scheduler.add_job(
        check_logs_and_retry,
        # æ¯å¤©æ‰§è¡Œä¸€æ¬¡
        trigger=CronTrigger.from_crontab('0 6,18,23 * * *'),
        id='é‡è¯•ä»»åŠ¡', 
        replace_existing=True,
        max_instances=10,
        misfire_grace_time=60*60*24
    )
    scheduler.add_job(
        backup_data,
        # æ¯å¤©æ‰§è¡Œä¸€æ¬¡
        trigger=CronTrigger.from_crontab('0 6 * * *'),
        id='å¤‡ä»½æ•°æ®', 
        replace_existing=True,
        max_instances=10,
        misfire_grace_time=60*60*24
    )
    logger.success("All DataX schedulers started successfully.")



class SystemMonitorAPI(APIView):
    def get(self, request):
        try:
            # è·å–ç³»ç»Ÿæ€§èƒ½æ•°æ®
            cpu_percent = psutil.cpu_percent(interval=1)
            mem = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            net_io = psutil.net_io_counters()
            
            data = {
                "timestamp": datetime.now().isoformat(),
                "cpu": {
                    "percent": cpu_percent,
                    "cores": psutil.cpu_count(logical=False)
                },
                "memory": {
                    "total": mem.total,
                    "used": mem.used,
                    "percent": mem.percent
                },
                "disk": {
                    "total": disk.total,
                    "used": disk.used,
                    "percent": disk.percent
                },
                "network": {
                    "sent": net_io.bytes_sent,
                    "recv": net_io.bytes_recv
                }
            }
            return Response(data)
        except Exception as e:
            return Response({"error": str(e)}, status=500)

class LogStatisticsAPI(APIView):
    def get(self, request):
        # è·å–ç­›é€‰å‚æ•°
        project = request.GET.get('project')
        start_date = request.GET.get('start_date')
        end_date = request.GET.get('end_date')
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        filters = Q()
        if project:
            filters &= Q(task__project__name=project)
        if start_date:
            try:
                start_date = datetime.strptime(start_date, '%Y-%m-%d')
                filters &= Q(created_at__gte=start_date)
            except ValueError:
                pass
        if end_date:
            try:
                end_date = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=1)
                filters &= Q(created_at__lte=end_date)
            except ValueError:
                pass

        # çŠ¶æ€ç»Ÿè®¡
        status_stats = Log.objects.filter(filters).values('executed_state').annotate(
            count=Count('id'),
            avg_time=Avg(F('end_time') - F('start_time'))
        ).order_by()
        # `end_time` datetime(6) DEFAULT NULL,
        # `start_time` datetime(6) DEFAULT NULL,
        # é¡¹ç›®ç»Ÿè®¡
        project_stats = Log.objects.filter(filters).values('task__project__name').annotate(
            success=Count('id', filter=Q(executed_state='success')),
            fail=Count('id', filter=Q(executed_state='fail')),
            running=Count('id', filter=Q(executed_state='process')),
            bak=Count('id', filter=Q(executed_state='bak')),
            stop=Count('id', filter=Q(executed_state='stop')),
            total=Count('id'),
            avg_time=Avg(
                ExpressionWrapper(F('local_row_update_time_end') - F('local_row_update_time_start'), output_field=DurationField())
            )
        ).order_by('task__project__name')
        
        # æ¯æ—¥ç»Ÿè®¡
        daily_stats = Log.objects.filter(filters).extra(
            select={'day': 'DATE(executors_log.updated_at)'}
        ).values('day').annotate(
            count=Count('id'),
            avg_time=Avg(
                ExpressionWrapper(F('local_row_update_time_end') - F('local_row_update_time_start'), output_field=DurationField())
            )
        ).order_by('day')

        # å¤„ç†çŠ¶æ€ç»Ÿè®¡ç»“æœ
        status_data = {
            'success': 0,
            'fail': 0,
            'running': 0,
            'bak': 0,
            'stop': 0,
        }
        for stat in status_stats:
            if stat['executed_state'] == 'success':
                status_data['success'] = stat['count']
            elif stat['executed_state'] == 'fail':
                status_data['fail'] = stat['count']
            elif stat['executed_state'] == 'process':
                status_data['running'] = stat['count']
            elif stat['executed_state'] == 'bak':
                status_data['bak'] = stat['count']
            elif stat['executed_state'] == 'stop':
                status_data['stop'] = stat['count']

        data = {
            'status_stats': status_data,
            'project_stats': list(project_stats),
            'daily_stats': list(daily_stats),
        }
        return Response(data, status=status.HTTP_200_OK)
def log_statistics(request):
    from .models import Project
    projects = Project.objects.all()
    monitor_data =SystemMonitorAPI().get(request).data
    return render(request, 'log_statistics.html',  {
        'projects': projects,
        'monitor_data': monitor_data
    })

# é‡å®šå‘adminè·¯ç”±
def redirect_to_admin(request):
    return redirect('/admin/')